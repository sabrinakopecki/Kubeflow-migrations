{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid-19 lung classification pipeline in Kubeflow\n",
    "\n",
    "In this notebook, the **Covid-19 lung classification notebook** is segmented into components and executed as a **Kubeflow pipeline** run. A pipeline is a description of an ML workflow that includes all of the steps in the form of components in the workflow. A pipeline component is a self-contained set of user code, packaged as a Docker image, that performs one step in the pipeline. For example, this can be a component responsible for data preprocessing, data transformation, model training, and so on. For a conventional data science notebook to run as a Kubeflow pipeline it has to be brought into a Kubeflow *friendly* format which this notebook is dedicated to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics](pics/lung_Kubeflow.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the new network (only the new classifier part) for the task of differentiating x-ray pictures of healthy lungs from x-ray pictures of covid-19 infected lungs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "import json\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath\n",
    ")\n",
    "import requests\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load resuable components, define data location & name, MinIO, and namespace\n",
    "\n",
    "Reusable components for repetitive steps are loaded in the first step. The components are located in a coworker's github as a **.yaml** file and have to be loaded using the url path. Kubeflow is designed to allow data scientists to reuse components when they execute a step of the ML workflow that happens frequently, for example downloading the data into the notebook. Other components that can be reused here are for model conversion, model upload and model deployment. The components for those steps are only compatible with models trained using *tensorflow*. For models using other frameworks, other components need to be loaded or the component needs to be defined in the notebook.\n",
    "\n",
    "The dataset used in this notebook was uploaded to the file hosting service box. The URL and file name is mentioned next as well as the model name. Kubeflow ships with MinIO inside to store all of its pipelines, artifacts and logs. The URL, username and password must be called here. \n",
    "\n",
    "Kubeflow comes with multi-user isolation which simplifies user operations because each user only views and edits the Kubeflow components and model artifacts defined in their configuration. Isolation uses Kubernetes **Namespaces**. The Namespace needs to be specified before the other steps of the pipeline can be defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user-example-com'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = [\n",
    "    \"Covid\",\n",
    "    \"Normal\"\n",
    "]\n",
    "\n",
    "\n",
    "DOWNLOAD_AND_EXTRACT_COMPONENT_URL = \"https://raw.githubusercontent.com/lehrig/kubeflow-ppc64le-components/main/data-extraction/download-and-extract-from-url/component.yaml\"\n",
    "CONVERT_MODEL_TO_ONNX_COMPONENT_URL = \"https://raw.githubusercontent.com/lehrig/kubeflow-ppc64le-components/main/model-building/convert-to-onnx/component.yaml\"\n",
    "UPLOAD_MODEL_COMPONENT_URL = \"https://raw.githubusercontent.com/lehrig/kubeflow-ppc64le-components/main/model-building/upload-model/component.yaml\"\n",
    "DEPLOY_MODEL_WITH_KSERVE_COMPONENT_URL = \"https://raw.githubusercontent.com/lehrig/kubeflow-ppc64le-components/main/model-deployment/deploy-model-with-kserve/component.yaml\"\n",
    "\n",
    "DATASET_URL = \"https://ibm.box.com/shared/static/5k8j40bj9niw4lqsslz9l4eydjj96s0w.zip\"\n",
    "DATASET_FILE_NAME = \"Covid_lungs.zip\"\n",
    "MODEL_NAME = \"covid-classification\"\n",
    "\n",
    "MINIO_URL = \"minio-service.kubeflow:9000\"\n",
    "MINIO_USER = \"minio\"\n",
    "MINIO_PASS = \"minio123\"\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "## 1.1 Load dataset\n",
    "\n",
    "The first component download the data and extracts it from a zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_comp = comp.load_component_from_url(\n",
    "    DOWNLOAD_AND_EXTRACT_COMPONENT_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing\n",
    "\n",
    "In the second component all the preprocessing is done before the data can be used to train the model. The data scientist has to decide which steps qualify as preprocessing steps and incorporates the code pieces into this component. In this example, the list of images are taken from the dataset directory and then the list of data and class images is initialized. After that the data and labels are converted to NumPy arrays. Finally, one-hot encoding is performed on the labels.\n",
    "\n",
    "Besides the preprocessing code, the component follows a clear logic where **Input** and **Output paths** are defined at the top, **packages & modules** are imported, **data** is imported, and after all the relevant code is inserted the data gets saved to a **new data directory** and the component receives a **base image** that contains all the relevant packages needed to run the code inside the component. This logic stays the same for every subsequent component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    data_dir:InputPath(str),\n",
    "    prep_dir: OutputPath(str)\n",
    "):\n",
    "    import os\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    from imutils import paths\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "    print(\"[INFO] loading images...\")\n",
    "    imagePaths = list(paths.list_images(data_dir))\n",
    "    print(\"length of imagePaths: \"+ str(len(imagePaths)))\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # loop over the image paths\n",
    "    for imagePath in imagePaths:\n",
    "        # extract the class label (directory-name) from the filename\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        print(\"label: \" + label)\n",
    "        if (label==\"covid\" or label==\"normal\"):\n",
    "            # load the image, swap color channels, and resize it to be a fixed\n",
    "            # 224x224 pixels while ignoring aspect ratio\n",
    "            image = cv2.imread(imagePath)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (224, 224))\n",
    "\n",
    "            # update the data and labels lists, respectively\n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "            \n",
    "    print(\"length of labels: \"+ str(len(labels))) \n",
    "    print(\"length of data: \"+ str(len(data)))\n",
    "\n",
    "    data = np.array(data) / 255.0\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    \n",
    "    print(\"length of labels: \"+ str(len(labels))) \n",
    "    print(\"length of data: \"+ str(len(data)))\n",
    "\n",
    "    if not os.path.exists(prep_dir):\n",
    "        os.makedirs(prep_dir)\n",
    "\n",
    "    np.savez(f'/{prep_dir}/data.npz', data=data, labels=labels)\n",
    "\n",
    "preprocess_data_comp = kfp.components.create_component_from_func(\n",
    "    func=preprocess_data,\n",
    "    packages_to_install=[\"imutils\"],\n",
    "    base_image='quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.7.0-py3.8-tensorflow-cpu2.7.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train test split\n",
    "The **data and labels** are saved to the *prep_dir* in the first component and are loaded again in the next component. This step takes care of the split of the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintestsplit_data(\n",
    "    prep_dir: InputPath(str),\n",
    "    file_name: str,\n",
    "    traintest_dir: OutputPath(str)\n",
    "):\n",
    "    \"\"\"Split data into train/validate/test data. Saves result into `prep_data_dir`.\"\"\"\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    data = np.load(f'/{prep_dir}/data.npz')['data']\n",
    "    labels = np.load(f'/{prep_dir}/data.npz')['labels']\n",
    "    \n",
    "    print(\"length of labels: \"+ str(len(labels))) \n",
    "    print(\"length of data: \"+ str(len(data)))\n",
    "    \n",
    "    (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.40, stratify=labels, random_state=42)\n",
    "\n",
    "    print(trainX.shape)\n",
    "    print(testX.shape)\n",
    "    print(trainY.shape)\n",
    "    print(testY.shape)\n",
    "\n",
    "    if not os.path.exists(traintest_dir):\n",
    "        os.makedirs(traintest_dir)\n",
    "\n",
    "    np.savez(f'/{traintest_dir}/train_data.npz', trainX, trainY)\n",
    "    np.savez(f'/{traintest_dir}/val_data.npz', testX, testY)\n",
    "\n",
    "    print(f'Data saved to {traintest_dir}:')\n",
    "    print(os.listdir(traintest_dir))\n",
    "\n",
    "traintestsplit_data_comp = kfp.components.create_component_from_func(\n",
    "    func=traintestsplit_data,\n",
    "    base_image='quay.io/ibm/kubeflow-component-tensorflow-cpu:latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Train the model\n",
    "In this component the model is trained and then it gets saved to the **model directory**. The **Image Data Generator** is also applied in this component. The [ImageDataGenerator](https://keras.io/api/preprocessing/image/) is an easy way to load and augment images in batches for image classification tasks. Together with the method `fit_generator()` (see below), it provides the possibility, that not all of the training data must be kept in the memory. Instead only the current batch is loaded. Moreover, the `ImageDataGenerator`-class provides methods to modify images, e.g. by shift, rotation, flipping, color-transform etc. \n",
    "In the code cell below an object of this class is instantiated, which will randomly rotate images within an angle of 15°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    traintest_dir: InputPath(str),\n",
    "    model_dir: OutputPath(str)\n",
    "):\n",
    "    \"\"\"Trains model. Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from tensorflow.keras.applications import VGG16\n",
    "    from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    \n",
    "    train_data = np.load(f'{traintest_dir}/train_data.npz')\n",
    "    trainX = train_data[train_data.files[0]]\n",
    "    trainY = train_data[train_data.files[1]]\n",
    "    \n",
    "    val_data = np.load(f'{traintest_dir}/val_data.npz')\n",
    "    testX = val_data[val_data.files[0]]\n",
    "    testY = val_data[val_data.files[1]]\n",
    "    \n",
    "    INIT_LR = 1e-3 #Initial Learning Rate\n",
    "    EPOCHS = 25 #Number of epochs in training\n",
    "    BS = 10 #Training Batch Size\n",
    "\n",
    "    trainAug = ImageDataGenerator(rotation_range=15, fill_mode=\"nearest\")\n",
    "\n",
    "    baseModel = VGG16(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "    headModel = baseModel.output\n",
    "    headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(64, activation=\"relu\")(headModel)\n",
    "    headModel = Dropout(0.5)(headModel)\n",
    "    headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "    print(\"[INFO] training classifier part of the network...\")\n",
    "    hist = model.fit_generator(\n",
    "        trainAug.flow(trainX, trainY, batch_size=BS),\n",
    "        steps_per_epoch=len(trainX) // BS,\n",
    "        validation_data=(testX, testY),\n",
    "        validation_steps=len(testX) // BS,\n",
    "        verbose=False,\n",
    "        epochs=EPOCHS)\n",
    "\n",
    "    print(\"Model train history:\")\n",
    "    print(hist.history)\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    model.save(model_dir)\n",
    "    print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "\n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    func=train_model,\n",
    "    output_component_file='train_model_component.yaml',\n",
    "    base_image='quay.io/ibm/kubeflow-component-tensorflow-gpu:2.7.0-dev'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Evaluate model with validation data\n",
    "This component does the evaluation of the model. The necessary packages and data from previously created directories are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    traintest_dir: InputPath(str),\n",
    "    model_dir: InputPath(str)  \n",
    "):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n",
    "    metadata.\"\"\"\n",
    "\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from collections import namedtuple\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    \n",
    "    val_data = np.load(f'{traintest_dir}/val_data.npz')\n",
    "    testX = val_data[val_data.files[0]]\n",
    "    testY = val_data[val_data.files[1]]\n",
    "    \n",
    "    BS = 10\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "    print(\"[INFO] Apply model on test data...\")\n",
    "    predIdxs = model.predict(testX, batch_size=BS)\n",
    "\n",
    "    predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "    print(predIdxs)\n",
    "\n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate_model,\n",
    "    output_component_file='evaluate_model_component.yaml',\n",
    "    base_image='quay.io/ibm/kubeflow-component-tensorflow-cpu:latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Create confusion matrix\n",
    "This pipeline uses a **confusion matrix** to visualize the performance of the model. To create a confusion matrix a single self-contained component must be defined following the same logic as the other components. The **model**, the **input data** and important **packages** need to be loaded first and then the confusion matrix is defined in detail. Like the other components, this one also needs a base image containing all the necessary modules to run the code inside the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "        traintest_dir: InputPath(str),\n",
    "        model_dir: InputPath(str),\n",
    "        labels: list,\n",
    "        mlpipeline_ui_metadata_path: OutputPath()):\n",
    "    import json\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import sys\n",
    "    import tensorflow as tf\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format='%(levelname)s %(asctime)s: %(message)s'\n",
    "    )\n",
    "    \n",
    "    val_data = np.load(f'{traintest_dir}/val_data.npz')\n",
    "    testX = val_data[val_data.files[0]]\n",
    "    testY = val_data[val_data.files[1]]\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "    y_true = np.argmax(testY, axis=1)\n",
    "    y_pred = np.argmax(model.predict(testX), axis=1)\n",
    "    confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(confusion_matrix):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((labels[target_index], labels[predicted_index], count))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=['target', 'predicted', 'count']\n",
    "    )\n",
    "\n",
    "    metadata = {\n",
    "      'outputs': [{\n",
    "        'type': 'confusion_matrix',\n",
    "        'format': 'csv',\n",
    "        'schema': [\n",
    "          {'name': 'target', 'type': 'CATEGORY'},\n",
    "          {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "          {'name': 'count', 'type': 'NUMBER'},\n",
    "        ],\n",
    "        \"storage\": \"inline\",\n",
    "        'source': df.to_csv(\n",
    "            columns=['target', 'predicted', 'count'],\n",
    "            header=False,\n",
    "            index=False),\n",
    "        'labels': labels,\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    logging.info(\"Dumping mlpipeline_ui_metadata...\")\n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    logging.info(\"Finished.\")\n",
    "\n",
    "\n",
    "plot_confusion_matrix_comp = kfp.components.create_component_from_func(\n",
    "    func=plot_confusion_matrix,\n",
    "    base_image='quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.7.0-py3.8-tensorflow-cpu2.7.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Convert model to ONNX (by reusing a Kubeflow component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_model_to_onnx_comp = kfp.components.load_component_from_url(\n",
    "    CONVERT_MODEL_TO_ONNX_COMPONENT_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Upload model to MinIO artifact store (by reusing a Kubeflow component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_model_comp = kfp.components.load_component_from_url(\n",
    "    UPLOAD_MODEL_COMPONENT_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Deploy the model using KServe (by reusing a Kubeflow component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_model_with_kserve_comp = kfp.components.load_component_from_url(\n",
    "    DEPLOY_MODEL_WITH_KSERVE_COMPONENT_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Pipeline\n",
    "After all the components have been specified, the pipeline is defined using the **@dsl.pipeline** decorator. The pipeline determines the succession of components to run and which parameters to pass between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Covid19 lung classification',\n",
    "  description='A pipeline that performs a Covid19 classification on lung images'\n",
    ")\n",
    "def covid19_lung_classification_pipeline(\n",
    "            labels: list,\n",
    "            dataset_url: str,\n",
    "            dataset_file_name: str = \"data.zip\",\n",
    "            data_dir: str = \"/train/data\",\n",
    "            prep_dir: str = \"/train/prep_data\",\n",
    "            traintest_dir: str = \"/train/traintest_data\",\n",
    "            model_dir: str = \"/train/model\",\n",
    "            model_name: str = \"covid-classification\",\n",
    "            minio_url: str = MINIO_URL,\n",
    "            minio_user: str = MINIO_USER,\n",
    "            minio_pass: str = MINIO_PASS):\n",
    "    \n",
    "    download_and_extract_task = download_and_extract_comp(\n",
    "        url=dataset_url,\n",
    "        file_name=dataset_file_name\n",
    "    )\n",
    "\n",
    "    preprocess_data_task = preprocess_data_comp(\n",
    "        download_and_extract_task.outputs['data_path']\n",
    "    )\n",
    "    \n",
    "    traintestsplit_data_task = traintestsplit_data_comp(\n",
    "        preprocess_data_task.outputs['prep_dir'],\n",
    "        file_name=dataset_file_name\n",
    "    )  \n",
    "    \n",
    "    train_model_task = train_model_comp(\n",
    "        traintestsplit_data_task.output\n",
    "    ).set_gpu_limit(1)\n",
    "\n",
    "    evaluate_model_task = evaluate_model_comp(\n",
    "        traintestsplit_data_task.output,\n",
    "        train_model_task.output\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix_task = plot_confusion_matrix_comp(\n",
    "        traintestsplit_data_task.output,\n",
    "        train_model_task.output,\n",
    "        labels\n",
    "    )\n",
    "\n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(\n",
    "        train_model_task.output\n",
    "    )\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        convert_model_to_onnx_task.output,\n",
    "        minio_url,\n",
    "        minio_user,\n",
    "        minio_pass,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    deploy_model_with_kserve_task = deploy_model_with_kserve_comp(\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    deploy_model_with_kserve_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Run the pipline within an experiment\n",
    "After defining the pipeline arguments the pipeline run is executed. Click on *Run details* which will appear below the cell and view the run of the pipeline inside the Kubeflow Pipelines UI opening in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/0b0d9a68-91e4-494a-a97f-92ac03cd98ea\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/1303620f-00ba-4dbe-8c82-41aed3e458c1\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=1303620f-00ba-4dbe-8c82-41aed3e458c1)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments = {\n",
    "    'labels': labels,\n",
    "    'dataset_url': DATASET_URL,\n",
    "    'dataset_file_name': DATASET_FILE_NAME,\n",
    "    'data_dir': '/train/data',\n",
    "    'prep_dir': '/train/prep_data',\n",
    "    'traintest_dir' : '/train/traintest_data',\n",
    "    'model_dir': '/train/model',\n",
    "    'model_name': MODEL_NAME,\n",
    "    'minio_url': MINIO_URL,\n",
    "    'minio_user': MINIO_USER,\n",
    "    'minio_pass': MINIO_PASS\n",
    "}\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    covid19_lung_classification_pipeline,\n",
    "    arguments=arguments,\n",
    "    namespace=NAMESPACE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
