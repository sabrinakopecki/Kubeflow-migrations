{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance charges regression pipeline in Kubeflow\n",
    "\n",
    "In this notebook, the **insurance charge regression notebook** is segmented into components and executed as a **Kubeflow pipeline** run. A pipeline is a description of an ML workflow that includes all of the steps in the form of components in the workflow. A pipeline component is a self-contained set of user code, packaged as a Docker image, that performs one step in the pipeline. For example, this can be a component responsible for data preprocessing, data transformation, model training, and so on. For a conventional data science notebook to run as a Kubeflow pipeline it has to be brought into a Kubeflow *friendly* format which this notebook is dedicated to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics](pics/insurance_Kubeflow.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load resuable components, define data location & name, MinIO, and namespace\n",
    "\n",
    "Reusable components for repetitive steps are loaded in the first step. The components are located in a coworker's github as a **.yaml** file and have to be loaded using the url path. Kubeflow is designed to allow data scientists to reuse components when they execute a step of the ML workflow that happens frequently, for example downloading the data into the notebook. The dataset used in this notebook was uploaded to the file hosting service box. The URL and file name is mentioned next as well as the model name. Kubeflow ships with MinIO inside to store all of its pipelines, artifacts and logs. The URL, username and password must be called here. \n",
    "\n",
    "Kubeflow comes with multi-user isolation which simplifies user operations because each user only views and edits the Kubeflow components and model artifacts defined in their configuration. Isolation uses Kubernetes **Namespaces**. The Namespace needs to be specified before the other steps of the pipeline can be defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user-example-com'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOWNLOAD_AND_EXTRACT_COMPONENT_URL = \"https://raw.githubusercontent.com/lehrig/kubeflow-ppc64le-components/main/data-extraction/download-and-extract-from-url/component.yaml\"\n",
    "\n",
    "DATASET_URL = \"https://ibm.box.com/shared/static/yqdpzhhe4x878hxcgu4a6uobra1dq7e9.zip\"\n",
    "DATASET_FILE_NAME = \"insurance.zip\"\n",
    "MODEL_NAME = \"insurance-cost-regression\"\n",
    "\n",
    "MINIO_URL = \"minio-service.kubeflow:9000\"\n",
    "MINIO_USER = \"minio\"\n",
    "MINIO_PASS = \"minio123\"\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important packages to build and run Kubeflow pipelines are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "from typing import NamedTuple\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "## 1.1 Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component download the data and extracts it from a zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_comp = comp.load_component_from_url(\n",
    "    DOWNLOAD_AND_EXTRACT_COMPONENT_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second component all the preprocessing is done before the data can be used to train the model. The data scientist has to decide which steps qualify as preprocessing steps and incorporates the code pieces into this component. In this example, the non-numerical features 'sex', 'smoker', 'region' are transformed into numerical features using the **Label Encoder**. After the preprocessing is done the data is saved to a new data directory called *prep_data_dir* as well as the dataframe which takes on the **.pkl** format. \n",
    "\n",
    "Besides the preprocessing code, the component follows a clear logic where **Input** and **Output paths** are defined at the top, **packages & modules** are imported, **data** is imported, and after all the relevant code is inserted the data gets saved to a **new data directory** and the component receives a **base image** that contains all the relevant packages needed to run the code inside the component. This logic stays the same for every subsequent component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19084f27aa806adea170b8aa8d31f41ef01e8f17"
   },
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "_uuid": "51e6ac44185ff301d77ff5c70358ed63a0cba055"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    data_dir:InputPath(str),\n",
    "    prep_data_dir: OutputPath(str)\n",
    "):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    data = f'{data_dir}/insurance.csv'\n",
    "    \n",
    "    insurancedf=pd.read_csv(data,na_values=[\" \",\"null\"])\n",
    "\n",
    "    catFeats=['sex','smoker','region']\n",
    "    for cf in catFeats:\n",
    "        print(\"\\nFeature %s :\"%cf)\n",
    "        print(insurancedf[cf].value_counts())\n",
    "\n",
    "    for cf in catFeats:\n",
    "        insurancedf[cf] = LabelEncoder().fit_transform(insurancedf[cf].values)\n",
    "\n",
    "    if not os.path.exists(prep_data_dir):\n",
    "        os.makedirs(prep_data_dir)\n",
    "\n",
    "    insurancedf.to_pickle(f'{prep_data_dir}/insurancedf.pkl')\n",
    "\n",
    "preprocess_data_comp = kfp.components.create_component_from_func(\n",
    "    func=preprocess_data,\n",
    "    base_image='quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.7.0-py3.8-tensorflow-cpu2.7.0',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **insurancedf** dataframe saved to the *prep_data_dir* in the first component is loaded again in the next component. This step takes care of the split of the training and testing data and also the model training. After the model is trained it gets saved to the **model directory**. The test and train splits also get saved so that they can be used in the next component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "_uuid": "2c0277fbf776834bb59da61eb5a73ce385a40209"
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    prep_data_dir: comp.InputPath(str),\n",
    "    model_dir: comp.OutputPath(str),\n",
    "    traintest_dir: comp.OutputPath(str)\n",
    "):\n",
    "    \"\"\"The train test split is done and then the model is trained\"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    insurancedf = pd.read_pickle(f'{prep_data_dir}/insurancedf.pkl')\n",
    "    \n",
    "    X=insurancedf.values[:,:-1]\n",
    "    y=insurancedf.values[:,-1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=0)\n",
    "    \n",
    "    linreg=LinearRegression()\n",
    "    linreg.fit(X_train,y_train)\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    if not os.path.exists(traintest_dir):\n",
    "        os.makedirs(traintest_dir)\n",
    "        \n",
    "    np.savez(f'{traintest_dir}/train_data.npz', X_train, y_train)\n",
    "    np.savez(f'{traintest_dir}/val_data.npz', X_test, y_test)\n",
    "    \n",
    "    filename = f'{model_dir}/finalized_model.sav'\n",
    "    pickle.dump(linreg, open(filename, 'wb'))\n",
    "    \n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    func=train_model,\n",
    "    base_image='quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.7.0-py3.8-tensorflow-cpu2.7.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Model evaluation\n",
    "\n",
    "The final component does the evaluation of the model. The necessary packages and data from previously created directories are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "_uuid": "5faca33e82001786e9c2fc83b6ec93fd2cca2059"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    traintest_dir: comp.InputPath(str),\n",
    "    model_dir: comp.InputPath(str),\n",
    "):\n",
    "\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    val_data = np.load(f'{traintest_dir}/val_data.npz')\n",
    "    X_test = val_data[val_data.files[0]]\n",
    "\n",
    "    model = pickle.load(open(f'{model_dir}/finalized_model.sav', 'rb'))\n",
    "\n",
    "    ypred=model.predict(X_test)\n",
    "\n",
    "    print(ypred)\n",
    "\n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate_model,\n",
    "    base_image='quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.7.0-py3.8-tensorflow-cpu2.7.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef08e0d5b86c994c01b64cf56b29a6039e607a48"
   },
   "source": [
    "# 2 Pipeline\n",
    "\n",
    "After all the components have been specified, the pipeline is defined using the **@dsl.pipeline** decorator. The pipeline determines the succession of components to run and which parameters to pass between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Insurance regression pipeline',\n",
    "  description='insurance regression ....'\n",
    ")\n",
    "def insurance_pipeline(dataset_url: str,\n",
    "                    dataset_file_name: str = \"data.zip\",\n",
    "                    data_dir: str = \"/train/data\",\n",
    "                    prep_data_dir: str = \"/train/prep_data\",\n",
    "                    model_dir: str = \"/train/model\",\n",
    "                    model_name: str = \"insurance-regression\",\n",
    "                    minio_url: str = MINIO_URL,\n",
    "                    minio_user: str = MINIO_USER,\n",
    "                    minio_pass: str = MINIO_PASS):\n",
    "    download_and_extract_task = download_and_extract_comp(\n",
    "        url=dataset_url,\n",
    "        file_name=dataset_file_name\n",
    "    )\n",
    "\n",
    "    preprocess_data_task = preprocess_data_comp(\n",
    "        download_and_extract_task.outputs['data_path']\n",
    "    )\n",
    "\n",
    "    train_model_task = train_model_comp(\n",
    "        preprocess_data_task.output\n",
    "    ).set_gpu_limit(1)\n",
    "\n",
    "    evaluate_model_task = evaluate_model_comp(\n",
    "        train_model_task.outputs['traintest_dir'],\n",
    "        train_model_task.outputs['model_dir']\n",
    "    ).set_gpu_limit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Run the pipeline\n",
    "\n",
    "After defining the pipeline arguments the pipeline run is executed. Click on *Run details* which will appear below the cell and view the run of the pipeline inside the Kubeflow Pipelines UI opening in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/0b0d9a68-91e4-494a-a97f-92ac03cd98ea\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/cf0ed133-9a16-4b43-bf8c-38acaac02ae5\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=cf0ed133-9a16-4b43-bf8c-38acaac02ae5)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify argument values for your pipeline run.\n",
    "arguments = {\n",
    "    'dataset_url': DATASET_URL,\n",
    "    'dataset_file_name': DATASET_FILE_NAME,\n",
    "    'data_dir': '/train/data',\n",
    "    'prep_data_dir': '/train/prep_data',\n",
    "    'model_dir': '/train/model',\n",
    "    'model_name': MODEL_NAME,\n",
    "    'minio_url': MINIO_URL,\n",
    "    'minio_user': MINIO_USER,\n",
    "    'minio_pass': MINIO_PASS\n",
    "}\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    insurance_pipeline,\n",
    "    arguments=arguments,\n",
    "    namespace=NAMESPACE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
